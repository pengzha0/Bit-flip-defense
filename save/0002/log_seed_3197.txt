save path : /home/cmax/users/zp/Bit-flip-defense/save/0002
{'data_path': '/home/cmax/users/zp/data/cifar10', 'dataset': 'cifar10', 'arch': 'resnet20_quan', 'epochs': 200, 'optimizer': 'Adam', 'test_batch_size': 128, 'learning_rate': 0.001, 'momentum': 0.9, 'decay': 0.0001, 'schedule': [80, 120], 'gammas': [0.1, 0.1], 'print_freq': 50, 'save_path': '/home/cmax/users/zp/Bit-flip-defense/save/0002', 'resume': '', 'start_epoch': 0, 'evaluate': False, 'fine_tune': False, 'model_only': False, 'ngpu': 1, 'gpu_id': 0, 'workers': 8, 'manualSeed': 3197, 'reset_weight': True, 'optimize_step': False, 'enable_bfa': True, 'attack_sample_size': 128, 'n_iter': 20, 'k_top': 100, 'checkpoint_path': '/home/cmax/users/zp/Bit-flip-defense/save/0002/model_best.pth.tar', 'use_cuda': True}
Random Seed: 3197
python version : 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0]
torch  version : 1.13.0
cudnn  version : 8500
=> creating model 'resnet20_quan'
=> network :
 CifarResNet(
  (conv_1_3x3): quan_Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): quan_Linear(in_features=64, out_features=10, bias=True)
)
=> do not use any checkpoint for resnet20_quan model
  **Test** Prec@1 89.710 Prec@5 99.710 Error@1 10.290
k_top is set to 100
Attack sample size is 128
**********************************
Iteration: [001/020]   Attack Time 0.260 (0.260)  [2024-01-26 02:41:32]
loss before attack: 0.0388
loss after attack: 0.1822
bit flips: 1
hamming_dist: 1
  **Test** Prec@1 86.800 Prec@5 99.420 Error@1 13.200
iteration Time 0.735 (0.735)
**********************************
Iteration: [002/020]   Attack Time 0.114 (0.187)  [2024-01-26 02:41:33]
loss before attack: 0.1822
loss after attack: 0.5854
bit flips: 2
hamming_dist: 2
  **Test** Prec@1 81.700 Prec@5 98.790 Error@1 18.300
iteration Time 0.786 (0.760)
**********************************
Iteration: [003/020]   Attack Time 0.142 (0.172)  [2024-01-26 02:41:34]
loss before attack: 0.5854
loss after attack: 1.0581
bit flips: 3
hamming_dist: 3
  **Test** Prec@1 73.990 Prec@5 97.050 Error@1 26.010
iteration Time 0.754 (0.758)
**********************************
Iteration: [004/020]   Attack Time 0.115 (0.158)  [2024-01-26 02:41:35]
loss before attack: 1.0581
loss after attack: 1.9967
bit flips: 4
hamming_dist: 4
  **Test** Prec@1 59.740 Prec@5 91.820 Error@1 40.260
iteration Time 0.926 (0.800)
**********************************
Iteration: [005/020]   Attack Time 0.119 (0.150)  [2024-01-26 02:41:36]
loss before attack: 1.9967
loss after attack: 3.2101
bit flips: 5
hamming_dist: 5
  **Test** Prec@1 43.810 Prec@5 82.430 Error@1 56.190
iteration Time 0.777 (0.796)
**********************************
Iteration: [006/020]   Attack Time 0.112 (0.144)  [2024-01-26 02:41:37]
loss before attack: 3.2101
loss after attack: 5.1589
bit flips: 6
hamming_dist: 6
  **Test** Prec@1 30.140 Prec@5 71.770 Error@1 69.860
iteration Time 0.719 (0.783)
**********************************
Iteration: [007/020]   Attack Time 0.112 (0.139)  [2024-01-26 02:41:37]
loss before attack: 5.1589
loss after attack: 7.2085
bit flips: 7
hamming_dist: 7
  **Test** Prec@1 22.370 Prec@5 61.730 Error@1 77.630
iteration Time 0.746 (0.777)
**********************************
Iteration: [008/020]   Attack Time 0.112 (0.136)  [2024-01-26 02:41:38]
loss before attack: 7.2085
loss after attack: 9.0323
bit flips: 8
hamming_dist: 8
  **Test** Prec@1 19.590 Prec@5 58.360 Error@1 80.410
iteration Time 0.751 (0.774)
**********************************
Iteration: [009/020]   Attack Time 0.116 (0.133)  [2024-01-26 02:41:39]
loss before attack: 9.0323
loss after attack: 11.8956
bit flips: 9
hamming_dist: 9
  **Test** Prec@1 17.440 Prec@5 54.720 Error@1 82.560
iteration Time 0.755 (0.772)
**********************************
Iteration: [010/020]   Attack Time 0.114 (0.132)  [2024-01-26 02:41:40]
loss before attack: 11.8956
loss after attack: 15.0948
bit flips: 10
hamming_dist: 10
  **Test** Prec@1 16.300 Prec@5 53.590 Error@1 83.700
iteration Time 0.829 (0.778)
**********************************
Iteration: [011/020]   Attack Time 0.116 (0.130)  [2024-01-26 02:41:41]
loss before attack: 15.0948
loss after attack: 18.6640
bit flips: 11
hamming_dist: 11
  **Test** Prec@1 15.760 Prec@5 52.750 Error@1 84.240
iteration Time 0.753 (0.775)
**********************************
Iteration: [012/020]   Attack Time 0.115 (0.129)  [2024-01-26 02:41:42]
loss before attack: 18.6640
loss after attack: 22.7715
bit flips: 12
hamming_dist: 12
  **Test** Prec@1 14.520 Prec@5 52.030 Error@1 85.480
iteration Time 0.745 (0.773)
**********************************
Iteration: [013/020]   Attack Time 0.114 (0.128)  [2024-01-26 02:41:43]
loss before attack: 22.7715
loss after attack: 27.4710
bit flips: 13
hamming_dist: 13
  **Test** Prec@1 13.370 Prec@5 51.530 Error@1 86.630
iteration Time 0.766 (0.772)
**********************************
Iteration: [014/020]   Attack Time 0.121 (0.127)  [2024-01-26 02:41:44]
loss before attack: 27.4710
loss after attack: 33.0852
bit flips: 14
hamming_dist: 14
  **Test** Prec@1 12.690 Prec@5 50.860 Error@1 87.310
iteration Time 0.759 (0.771)
**********************************
Iteration: [015/020]   Attack Time 0.144 (0.128)  [2024-01-26 02:41:45]
loss before attack: 33.0852
loss after attack: 39.0874
bit flips: 15
hamming_dist: 15
  **Test** Prec@1 12.400 Prec@5 50.510 Error@1 87.600
iteration Time 0.749 (0.770)
**********************************
Iteration: [016/020]   Attack Time 0.113 (0.127)  [2024-01-26 02:41:45]
loss before attack: 39.0874
loss after attack: 46.6475
bit flips: 16
hamming_dist: 16
  **Test** Prec@1 12.230 Prec@5 50.640 Error@1 87.770
iteration Time 0.762 (0.769)
**********************************
Iteration: [017/020]   Attack Time 0.115 (0.127)  [2024-01-26 02:41:46]
loss before attack: 46.6475
loss after attack: 54.7239
bit flips: 17
hamming_dist: 17
  **Test** Prec@1 11.560 Prec@5 50.640 Error@1 88.440
iteration Time 0.757 (0.769)
**********************************
Iteration: [018/020]   Attack Time 0.115 (0.126)  [2024-01-26 02:41:47]
loss before attack: 54.7239
loss after attack: 63.4683
bit flips: 18
hamming_dist: 18
  **Test** Prec@1 11.260 Prec@5 50.400 Error@1 88.740
iteration Time 0.777 (0.769)
**********************************
Iteration: [019/020]   Attack Time 0.116 (0.126)  [2024-01-26 02:41:48]
loss before attack: 63.4683
loss after attack: 74.3331
bit flips: 19
hamming_dist: 19
  **Test** Prec@1 11.120 Prec@5 49.900 Error@1 88.880
iteration Time 0.791 (0.770)
**********************************
Iteration: [020/020]   Attack Time 0.113 (0.125)  [2024-01-26 02:41:49]
loss before attack: 74.3331
loss after attack: 85.4092
bit flips: 20
hamming_dist: 20
  **Test** Prec@1 10.810 Prec@5 49.900 Error@1 89.190
iteration Time 0.755 (0.770)
